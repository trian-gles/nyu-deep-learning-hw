{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "153cE-4dJbu9ygvqQxxWdlo8VoBatvdEG",
      "authorship_tag": "ABX9TyPjXfJYYRvRBr6MJRNnSFO+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trian-gles/nyu-deep-learning-hw/blob/main/hw_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n"
      ],
      "metadata": {
        "id": "vZ2B6KEXb3Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc2Wu4O6YXIN"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    _nonlinears_fwd = {\n",
        "            'relu': lambda t: torch.fmax(t, torch.zeros(t.size())),\n",
        "            'sigmoid': lambda t: (torch.ones(t.size()) + torch.exp(-1 * t)).pow(-1),\n",
        "            'identity': lambda t: t\n",
        "\n",
        "        }\n",
        "\n",
        "    _nonlinears_back = {\n",
        "        'relu': lambda t: (t > 0).to(int),\n",
        "        'sigmoid': lambda t: MLP._nonlinears_fwd['sigmoid'](t) * (torch.ones(t.size()) - MLP._nonlinears_fwd['sigmoid'](t)),\n",
        "        'identity': lambda t: torch.ones(t.size())\n",
        "    }\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        linear_1_in_features,\n",
        "        linear_1_out_features,\n",
        "        f_function,\n",
        "        linear_2_in_features,\n",
        "        linear_2_out_features,\n",
        "        g_function\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            linear_1_in_features: the in features of first linear layer\n",
        "            linear_1_out_features: the out features of first linear layer\n",
        "            linear_2_in_features: the in features of second linear layer\n",
        "            linear_2_out_features: the out features of second linear layer\n",
        "            f_function: string for the f function: relu | sigmoid | identity\n",
        "            g_function: string for the g function: relu | sigmoid | identity\n",
        "        \"\"\"\n",
        "        self.f_function = f_function\n",
        "        self.g_function = g_function\n",
        "\n",
        "        self.parameters = dict(\n",
        "            W1 = torch.randn(linear_1_out_features, linear_1_in_features),\n",
        "            b1 = torch.randn(linear_1_out_features),\n",
        "            W2 = torch.randn(linear_2_out_features, linear_2_in_features),\n",
        "            b2 = torch.randn(linear_2_out_features),\n",
        "        )\n",
        "        self.grads = dict(\n",
        "            dJdW1 = torch.zeros(linear_1_out_features, linear_1_in_features),\n",
        "            dJdb1 = torch.zeros(linear_1_out_features),\n",
        "            dJdW2 = torch.zeros(linear_2_out_features, linear_2_in_features),\n",
        "            dJdb2 = torch.zeros(linear_2_out_features),\n",
        "        )\n",
        "\n",
        "        # put all the cache value you need in self.cache\n",
        "        self.cache = dict()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: tensor shape (batch_size, linear_1_in_features)\n",
        "        \"\"\"\n",
        "\n",
        "        self.cache['lin_1_in'] = x # batch size, linear_1_in_features\n",
        "        lin_1_out = torch.matmul(x, torch.t(self.parameters[\"W1\"])) + self.parameters[\"b1\"] # batch size, limear_1_out_features\n",
        "                                # [batch size X lin_1_in] X [lin_1_in X lin_1_out] + [lin_1_out] = [batch size x lin_1_out]\n",
        "        self.cache['lin_1_out'] = lin_1_out\n",
        "        f_out = self._nonlinears_fwd[self.f_function](lin_1_out) # batch size, lin_1_out\n",
        "\n",
        "        self.cache['lin_2_in'] = f_out # batch size, lin_1_out\n",
        "        lin_2_out = torch.matmul(f_out, torch.t(self.parameters[\"W2\"])) + self.parameters[\"b2\"] # batch size, linear_2_out_features\n",
        "                                #[batch size X lin_1_out] X [lin_1_out X lin_2_out] + [lin_2_out] = [batch size x lin_2_out]\n",
        "        self.cache['lin_2_out'] = lin_2_out # batch size, linear_2_out_features\n",
        "\n",
        "        g_out = self._nonlinears_fwd[self.g_function](lin_2_out) # batch size, linear_2_out_features\n",
        "        return g_out\n",
        "    \n",
        "    def backward(self, dJdy_hat):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
        "        \"\"\"\n",
        "        dJdgin = dJdy_hat * self._nonlinears_back[self.g_function](self.cache['lin_2_out']) # batch size, linear_2_out_features\n",
        "        self.grads['dJdW2'] = torch.matmul(torch.t(dJdgin), self.cache['lin_2_in']) # batch size, linear_2_out_features\n",
        "#                                          [lin2outfeat x batch size] X [batch size x lin2infeat] = [lin2out X lin2in]\n",
        "        self.grads['dJdb2'] = torch.matmul(torch.ones(dJdgin.shape[0]), dJdgin) # [lin2outfeat]\n",
        "          # [batch size X lin2out] = [lin2outfeat]\n",
        "\n",
        "        dJdfout = torch.matmul(dJdgin, self.parameters['W2'])\n",
        "                              # [batch size X lin2out] X [lin2out X lin2in] = [batch size X lin2in(lin1out)]\n",
        "\n",
        "\n",
        "        dJdfin = dJdfout * self._nonlinears_back[self.f_function](self.cache['lin_1_out'])\n",
        "\n",
        "        self.grads['dJdW1'] = torch.matmul(torch.t(dJdfin), self.cache['lin_1_in'])\n",
        "                              # [lin1out X batch size] X [batch size x lin1in] = [lin1out x lin1in]\n",
        "\n",
        "        self.grads['dJdb1'] = torch.matmul(torch.ones(dJdfin.shape[0]), dJdfin) # trick to sum up all of the batches\n",
        "\n",
        "    \n",
        "    def clear_grad_and_cache(self):\n",
        "        for grad in self.grads:\n",
        "            self.grads[grad].zero_()\n",
        "        self.cache = dict()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing pieces"
      ],
      "metadata": {
        "id": "zX5_FiZqwd-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare(torch_func, my_forw, my_backw):\n",
        "  rand_els = torch.randn(1, 1, requires_grad=True)\n",
        "\n",
        "  my_output = my_forw(rand_els)\n",
        "  torch_output = torch_func(rand_els)\n",
        "\n",
        "  my_back = my_backw(rand_els)\n",
        "\n",
        "  torch_output.backward()\n",
        "  torch_back = rand_els.grad\n",
        "\n",
        "  print(my_output, torch_output)\n",
        "  print(my_back, torch_back)"
      ],
      "metadata": {
        "id": "tEbdbgeJxkgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare(torch.nn.Identity(), MLP._nonlinears_fwd['identity'], MLP._nonlinears_back['identity'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhyVpMkgv7in",
        "outputId": "9a0a9880-281c-422a-f680-803dbe4769ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1367]], requires_grad=True) tensor([[0.1367]], requires_grad=True)\n",
            "tensor([[1.]]) tensor([[1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_loss(y, y_hat):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        y: the label tensor (batch_size, linear_2_out_features)\n",
        "        y_hat: the prediction tensor (batch_size, linear_2_out_features)\n",
        "\n",
        "    Return:\n",
        "        J: scalar of loss\n",
        "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
        "    \"\"\"\n",
        "    loss = (torch.square(y_hat - y)).sum() / (y.size(dim=0) * y.size(dim=1))\n",
        "    dJdy_hat = 2 * (y_hat - y) / (y.size(dim=0) * y.size(dim=1))\n",
        "    # return loss, dJdy_hat\n",
        "\n",
        "    return loss, dJdy_hat\n",
        "\n"
      ],
      "metadata": {
        "id": "Zb-lzEOrZIMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VAFk13RLmGe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "y_hat = torch.randn(3, 5, requires_grad=True)\n",
        "y = torch.randn(3, 5)\n",
        "print(mse_loss(y, y_hat))\n",
        "\n",
        "torch_mse_loss = torch.nn.MSELoss()\n",
        "output = torch_mse_loss(y_hat, y)\n",
        "print(output)\n",
        "output.backward()\n",
        "print(y_hat.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQsE7Y6Hfu67",
        "outputId": "0c2b720c-cd98-4109-916e-0ff64a3641bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(1.0971, grad_fn=<DivBackward0>), tensor([[ 0.1211,  0.2152, -0.1248,  0.0567, -0.0976],\n",
            "        [-0.1469,  0.0817, -0.1453, -0.0113, -0.1360],\n",
            "        [-0.2298, -0.1878,  0.0012, -0.2166,  0.0171]], grad_fn=<DivBackward0>))\n",
            "tensor(1.0971, grad_fn=<MseLossBackward0>)\n",
            "tensor([[ 0.1211,  0.2152, -0.1248,  0.0567, -0.0976],\n",
            "        [-0.1469,  0.0817, -0.1453, -0.0113, -0.1360],\n",
            "        [-0.2298, -0.1878,  0.0012, -0.2166,  0.0171]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bce_loss(y, y_hat):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        y_hat: the prediction tensor\n",
        "        y: the label tensor\n",
        "        \n",
        "    Return:\n",
        "        loss: scalar of loss\n",
        "        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)\n",
        "    \"\"\"\n",
        "    loss = torch.mean(y * torch.log(y_hat) + (torch.ones(y.shape) - y) * torch.log(torch.ones(y.shape) - y_hat)) * -1\n",
        "    dJdy_hat = (y_hat - y) / ((y_hat * (torch.ones(y_hat.shape) - y_hat)) * y_hat.shape[0] * y_hat.shape[1])\n",
        "\n",
        "    return loss, dJdy_hat"
      ],
      "metadata": {
        "id": "F-sIPyEKZSeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = torch.rand(3, 5, requires_grad=True)\n",
        "y = torch.rand(3, 5)\n",
        "my_loss, my_d = bce_loss(y, y_hat)\n",
        "print(my_loss)\n",
        "\n",
        "torch_bce_loss = torch.nn.BCELoss()\n",
        "output = torch_bce_loss(y_hat, y)\n",
        "print(output)\n",
        "output.backward()\n",
        "print(my_d)\n",
        "print(y_hat.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4tLJLpV86KA",
        "outputId": "5347f37b-5a07-44a4-d3c3-8ed83a67a35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9840, grad_fn=<MulBackward0>)\n",
            "tensor(0.9840, grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "tensor([[-0.1704,  0.2386, -0.2822, -0.1741,  0.1928],\n",
            "        [-0.0993, -0.0063, -0.1474,  0.0866, -0.2359],\n",
            "        [-0.1852, -0.0878,  0.1056,  0.1814, -0.2266]], grad_fn=<DivBackward0>)\n",
            "tensor([[-0.1704,  0.2386, -0.2822, -0.1741,  0.1928],\n",
            "        [-0.0993, -0.0063, -0.1474,  0.0866, -0.2359],\n",
            "        [-0.1852, -0.0878,  0.1056,  0.1814, -0.2266]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 1"
      ],
      "metadata": {
        "id": "X6uLA7pqiXQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "dzweRZz6jpg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = MLP(\n",
        "    linear_1_in_features=2,\n",
        "    linear_1_out_features=20,\n",
        "    f_function='relu',\n",
        "    linear_2_in_features=20,\n",
        "    linear_2_out_features=5,\n",
        "    g_function='identity'\n",
        ")\n",
        "x = torch.randn(10, 2)\n",
        "y = torch.randn(10, 5)\n",
        "\n",
        "net.clear_grad_and_cache()\n",
        "y_hat = net.forward(x)\n",
        "J, dJdy_hat = mse_loss(y, y_hat)\n",
        "net.backward(dJdy_hat)\n",
        "\n",
        "#------------------------------------------------\n",
        "# compare the result with autograd\n",
        "net_autograd = nn.Sequential(\n",
        "    OrderedDict([\n",
        "        ('linear1', nn.Linear(2, 20)),\n",
        "        ('relu', nn.ReLU()),\n",
        "        ('linear2', nn.Linear(20, 5)),\n",
        "    ])\n",
        ")\n",
        "net_autograd.linear1.weight.data = net.parameters['W1']\n",
        "net_autograd.linear1.bias.data = net.parameters['b1']\n",
        "net_autograd.linear2.weight.data = net.parameters['W2']\n",
        "net_autograd.linear2.bias.data = net.parameters['b2']\n",
        "\n",
        "y_hat_autograd = net_autograd(x)\n",
        "\n",
        "J_autograd = F.mse_loss(y_hat_autograd, y)\n",
        "\n",
        "net_autograd.zero_grad()\n",
        "J_autograd.backward()\n",
        "\n",
        "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm() < 1e-3)\n",
        "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
        "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm() < 1e-3)\n",
        "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm()< 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW4snJetiZg8",
        "outputId": "69c1e215-d11c-4c2d-b7ac-24d113c15193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 2"
      ],
      "metadata": {
        "id": "kH7-vUod_lcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = MLP(\n",
        "    linear_1_in_features=2,\n",
        "    linear_1_out_features=20,\n",
        "    f_function='sigmoid',\n",
        "    linear_2_in_features=20,\n",
        "    linear_2_out_features=5,\n",
        "    g_function='sigmoid'\n",
        ")\n",
        "x = torch.randn(10, 2)\n",
        "y = (torch.randn(10, 5) < 0.5) * 1.0\n",
        "\n",
        "net.clear_grad_and_cache()\n",
        "y_hat = net.forward(x)\n",
        "J, dJdy_hat = bce_loss(y, y_hat)\n",
        "net.backward(dJdy_hat)\n",
        "\n",
        "#------------------------------------------------\n",
        "# compare the result with autograd\n",
        "net_autograd = nn.Sequential(\n",
        "    OrderedDict([\n",
        "        ('linear1', nn.Linear(2, 20)),\n",
        "        ('sigmoid1', nn.Sigmoid()),\n",
        "        ('linear2', nn.Linear(20, 5)),\n",
        "        ('sigmoid2', nn.Sigmoid()),\n",
        "    ])\n",
        ")\n",
        "net_autograd.linear1.weight.data = net.parameters['W1']\n",
        "net_autograd.linear1.bias.data = net.parameters['b1']\n",
        "net_autograd.linear2.weight.data = net.parameters['W2']\n",
        "net_autograd.linear2.bias.data = net.parameters['b2']\n",
        "\n",
        "y_hat_autograd = net_autograd(x)\n",
        "\n",
        "J_autograd = torch.nn.BCELoss()(y_hat_autograd, y)\n",
        "\n",
        "net_autograd.zero_grad()\n",
        "J_autograd.backward()\n",
        "\n",
        "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm() < 1e-3)\n",
        "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
        "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm() < 1e-3)\n",
        "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm()< 1e-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMHAJB02_k_z",
        "outputId": "2cf61dd1-2eef-4731-9a42-aa782d2a4d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 3"
      ],
      "metadata": {
        "id": "3osDsQiM_8Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = MLP(\n",
        "    linear_1_in_features=2,\n",
        "    linear_1_out_features=20,\n",
        "    f_function='relu',\n",
        "    linear_2_in_features=20,\n",
        "    linear_2_out_features=22,\n",
        "    g_function='relu'\n",
        ")\n",
        "x = torch.randn(10, 2)\n",
        "y = torch.randn(10, 22)\n",
        "\n",
        "net.clear_grad_and_cache()\n",
        "y_hat = net.forward(x)\n",
        "J, dJdy_hat = mse_loss(y, y_hat)\n",
        "net.backward(dJdy_hat)\n",
        "\n",
        "#------------------------------------------------\n",
        "# compare the result with autograd\n",
        "net_autograd = nn.Sequential(\n",
        "    OrderedDict([\n",
        "        ('linear1', nn.Linear(2, 20)),\n",
        "        ('relu1', nn.ReLU()),\n",
        "        ('linear2', nn.Linear(20, 22)),\n",
        "        ('relu2', nn.ReLU()),\n",
        "    ])\n",
        ")\n",
        "net_autograd.linear1.weight.data = net.parameters['W1']\n",
        "net_autograd.linear1.bias.data = net.parameters['b1']\n",
        "net_autograd.linear2.weight.data = net.parameters['W2']\n",
        "net_autograd.linear2.bias.data = net.parameters['b2']\n",
        "\n",
        "y_hat_autograd = net_autograd(x)\n",
        "\n",
        "J_autograd = F.mse_loss(y_hat_autograd, y)\n",
        "\n",
        "net_autograd.zero_grad()\n",
        "J_autograd.backward()\n",
        "\n",
        "print((net_autograd.linear1.weight.grad.data - net.grads['dJdW1']).norm() < 1e-3)\n",
        "print((net_autograd.linear1.bias.grad.data - net.grads['dJdb1']).norm() < 1e-3)\n",
        "print((net_autograd.linear2.weight.grad.data - net.grads['dJdW2']).norm() < 1e-3)\n",
        "print((net_autograd.linear2.bias.grad.data - net.grads['dJdb2']).norm()< 1e-3)\n",
        "#------------------------------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaXsO4HL_vqe",
        "outputId": "844d2d25-dfe3-4afb-825a-2eee31676b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n",
            "tensor(True)\n"
          ]
        }
      ]
    }
  ]
}